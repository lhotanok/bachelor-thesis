%%% Fiktivní kapitola s ukázkami sazby

\chapter{Analýza}

V~této kapitole si zadefinujeme požadavky na funkcionalitu naší aplikace. Také se v kontextu požadavků podíváme na existující webové stránky s~recepty a~provedeme diskuzi nad jejich funkcemi, možnými vylepšeními a~rozšířeními. Následně si rozebereme různé alternativy dostupných datových sad a srovnáme jejich výhody i~nevýhody vzhledem k~požadavkům aplikace.

\section{Požadavky aplikace}

\section{Dostupné datové sady}

V této sekci je vyhrazen prostor pro analýzu různých veřejně dostupných datasetů z domény receptů. Nejedná se ani zdaleka o kompletní výčet, měly by ale být představeny nejznámější alternativy, které by mohly být vybrány jako podklad pro obsah aplikaci. 

\subsection{Recipe1M+}

V~první fázi analýzy se zaměříme na veřejně dostupná zdrojová data s~recepty, která by mohla posloužit jako podklad pro naši databázi. Jedním z~nejdůležitějších projektů v~této oblasti je \emph{Recipe1M+}, strukturovaný korpus obsahující přes $1$~milion receptů a $13$ milionů souvisejících obrázků jídla. Aktuálně se jedná o~největší veřejně dostupnou sadu receptů. Dataset je dostupný pouze přihlášeným uživatelům z~ověřené organizace a je povoleno jej využívat výhradně pro účely studia a výzkumu. Z~celkového počtu $1$~milionu receptů obsahuje $50\,000$ receptů s~nutričními informacemi \citep{marin2019learning}. V~naší aplikaci preferujeme nutriční hodnoty zahrnout, pokud jsou dostupné na zdrojové stránce receptu. Měli bychom tedy k~dispozici $50\,000$ dokumentů s~touto informací. Ostatní data jsou určena přednostně pro strojové zpracování prostřednictvím trénování modelů.

Celková velikost datové sady se pohybuje v~řádu stovek gigabytů, samotné JSON dokumenty se strukturovanými recepty z~adresáře \texttt{layers} se ale vejdou do $2~GiB$, tudíž by byly vhodné pro potřeby této práce limitované omezenou výpočetní kapacitou. Lze odtud využít $1\,029\,720$ receptů obsahujících název, url, ingredience a~postup přípravy. Odkazy na ilustrační fotografie jsou u~$402\,760$ z~těchto receptů. Pro příjemnější uživatelský zážitek se omezujeme pouze na recepty s~obrázky, takže jsme z~datasetu Recipe1M+ schopni použít přibližně $400\,000$ receptů, pokud akceptujeme absenci nutričních hodnot. Bylo by spíše obtížnější z~tohoto datasetu identifikovat názvy ingrediencí, neboť jsou suroviny uloženy včetně jejich množství a~jednotek měření v~rozmanitém formátu.

\subsection{Open Recipes}

Dalším významným aktérem na poli volně dostupných receptů je iniciativa \emph{Open Recipes}. Autoři Finkler, Shiflett a Birkebæk projekt představují jako otevřenou databázi záložek s~recepty. Pojem záložky je použit z~důvodu absence instrukcí k~přípravě receptu. Dataset má sloužit pouze k~vyhledání receptu a~pro detailní informace má být uživatel přesměrován na zdroj s~kompletním receptem \citep{open-recipes}. Tohoto přístupu úspěšně využívají některé z~vyhledávačů receptů, např. populární aplikace \emph{SuperCook}. Naše aplikace si ale klade za cíl zpracovat i~stránky s~detaily receptů, ze kterých lze dále pokračovat na detaily ingrediencí s~informacemi ze znalostních grafů. Projekt Open Recipes tedy pro náš scénář nebude vhodnou volbou.

\subsection{Food.com Recipes and Interactions}

Rozsáhlý dataset \emph{Food.com Recipes and Interactions} s~téměř $200\,000$ recepty extrahovanými z~webové stránky \emph{Food.com} (původního GeniusKitchen) je publikován na portálu \emph{Kaggle}, který shromažďuje podklady pro strojové učení. Datová sada pokrývá $18$ let interakce uživatelů včetně hodnocení, počtu recenzí i~konkrétních reakcí \citep{shuyang_li_2019}. Kromě základních informací obsahuje také nutriční hodnoty receptů, datum publikování a~rovněž normalizovaná jména ingrediencí. Ta byla získána parsováním originálního textu surovin, kvůli čemuž nejsou vždy zcela spolehlivě přesná (např. ve jménech často zůstala jednotka měření z~původního textu). Unikátních ingrediencí je k~dispozici kolem $8\,000$, což by měl být dostačující základ pro hledání linků s~entitami otevřených znalostních grafů. Zároveň ve srovnání s~předchozími projekty nabízí nejbohatší informace k jednotlivým receptům.

Nevýhodou datasetu je jeho primární určení pro strojové zpracování. Byl vytvořen jako podklad pro generování personalizovaných receptů na základě dřívějších preferencí uživatele \citep{majumder-etal-2019-generating}. Syrová data nejsou zamýšlena pro přímou prezentaci, což se negativně odráží na jejich přesnosti a estetice. Slova jsou občas zařazena do špatných kategorií a~problematický je zejména plně \emph{lowercase} formát textu, ze kterého nejsme schopni zpětně zrekonstruovat originální text receptu. Dataset bychom tedy nemohli použít samostatně, ale pouze v~kombinaci s~vlastní extrakcí dat, která by respektovala velikost písma a~lépe se vypořádala s~parsováním jednotlivých kategorií.

Tento problém je poměrně snadno řešitelný díky struktuře stránky Food.com. Z~id receptu lze jednoduše složit url ve formátu \texttt{www.food.com/recipe/id} a~navíc aplikace podporuje koncept propojených dat, tedy poskytuje recepty ve strukturovaném RDF formátu. Do HTML hlaviček všech dokumentů s~recepty vkládá JSON-LD serializaci dle ontologie \emph{Schema.org}. Z~připraveného datasetu bychom tedy mohli využít identifikátory receptů a~normalizované ingredience, pro každý recept extrahovat jeho JSON-LD a spojit informace dohromady. Zároveň bychom si ušetřili práci s~převáděním receptů do JSON-LD formátu a připravené soubory rovnou vložili do hlaviček dokumentů. Nevytvářeli bychom nové entity receptů, pouze bychom změnili prezentační vrstvu RDF dat. Identifikátory entit v podobě IRI by tedy zůstaly nezměněné.

\subsection{FoodKG}

Přímo v oblasti znalostních grafů figuruje projekt \emph{FoodKG}, který je postaven nad sadou receptů z již zmíněného datasetu Recipe1M+. Recepty doplňuje o podrobnější data k ingrediencím ze stránky The Cook's Thesaurus a definuje vlastní ontologii. Model ontologie je navržen pro zodpovídání dotazů na recepty dle ingrediencí s přihlédnutím k individuálním potřebám uživatele, jako jsou alergie a intolerance na různé složky potravin.

Vývojáři projektu zpřístupňují skripty k extrakci dat z encyklopedie The Cook's Thesaurus a k vytvoření znalostního grafu. Neposkytují ale žádné nové recepty nad rámec datové sady Recipe1M+, naší horní hranicí by tedy bylo $50\,000$ receptů s nutričními hodnotami. Ontologie publikovaná na webových stránkách projektu obsahuje $75$ entit ingrediencí, které kromě obecného popisu poskytují informace o glykemickém indexu, obsahu lepku a možných náhradách dané ingredience. Výhodou je připravený RDF formát, nad kterým se lze snadno dotazovat pomocí jazyka SPARQL. Autoři Chen a kol. uvádějí ukázky dotazů, vyberme například dotaz vracející recepty, které obsahují banán a zároveň neobsahují vlašské ořechy \emph{FoodKG}:

\begin{code}
@PREFIX food: <http://purl.org/heals/food/>
@PREFIX ingredient: <http://purl.org/heals/ingredient/>
SELECT DISTINCT ?recipe
WHERE {
    ?recipe food:hasIngredient ingredient:Banana .
    FILTER NOT EXISTS {
        ?recipe food:hasIngredient ingredient:Walnut .
    }
}
\end{code}
%$

\subsection{Generování vlastního datasetu}

Pokud bychom se nespokojili s žádnou z dostupných datových sad, případně potřebovali data rozšířit a posbírat je přímo ze zdroje, využijeme metodu zvanou \emph{web scraping}. V rámci tohoto procesu musíme analyzovat cílovou stránku z pohledu získávání a prezentace dat. S využitím vývojářským nástrojů ve webovém prohlížeči můžeme přes panel \texttt{Network} sledovat požadavky, které aplikace odesílá na svůj server a v mnoha případech se na toto interní API dokážeme napojit a získat data ve strukturované podobě. Aplikace typicky pracují s REST API nebo GraphQL API a standardně data poskytují ve formátu JSON. Pokud žádný fetch request pro získávání potřebných dat neobjevíme, musíme informace extrahovat přímo z HTML dokumentu prostřednictvím CSS selektorů. V obou případech budeme aplikaci posílat GET requesty, ať už na její backend pro strukturovaná data nebo na frontend pro HTML dokumenty k následnému parsování.

Problematická je kategorie single-page aplikací, které data nezískávají s využitím transparentních fetch requestů a zároveň potřebují JavaScript pro vygenerování obsahu. Zde nestačí pouhé poslání GET requestu přes HTTP, neboť odpověď neobsahuje žádná relevantní data uvnitř HTML. Pro zvládnutí takovýchto stránek potřebujeme zapojit automatizaci webového prohlížeče. Nejznámější projekty, které se této automatizaci věnují, jsou Selenium, Puppeteer, Playwright a Cypress \citep{selenium-ecosystem}. Navíc můžeme během posílání requestů narazit na různé formy blokování, od limitu maximálního počtu requestů z jedné IP adresy přes povinné autorizační tokeny až po captcha testy řešitelné pouze s využitím umělé inteligence. Taky se při neopatrnosti může stát, že server aplikace zahltíme příliš velkým množstvím paralelních requestů, čímž prodloužíme dobu odezvy nebo zpracování dalších requestů dočasně zcela znemožníme. Stejně jako v jiných oblastech se hodí využít nástroj, který co nejvíce běžných problémů vyřeší za nás. Pro potřeby této práce by vzhledem k rozsáhlejší osobní zkušenosti byla vhodná open source knihovna Apify, která má v arzenálu zpracování HTTP requestů s následným parsováním HTML pomocí knihovny Cheerio, ale také automatizaci webového prohlížeče s využitím knihoven Puppeteer nebo Playwright. Navíc nabízí správu rotace IP adres, čímž snižuje množství zablokovaných requestů. IP adresy lze v placeném plánu získat přímo od firmy Apify, nebo na vstupu poskytnout seznam vlastních. Obecně nechceme program spouštět z vlastní IP adresy, neboť riskujeme, že nás stránka někdy i natrvalo zablokuje.
  