%%% Fiktivní kapitola s ukázkami citací

\chapter{Architektura řešení}

V~této kapitole se budeme věnovat návrhu architektury od extrakce a~uložení dat přes přípravu vyhledávacích indexů až po prezentaci těchto dat v~rámci funkční aplikace. Předzpracování dat bude klíčovou fází řešení spolu s~prezentační vrstvou na straně klienta. Serverová část aplikace bude vystupovat pouze jako prostředník mezi frontendem a~databází, respektive platformou Solr pro vyhledávací dotazy. Jejím úkolem bude zprostředkování požadovaných dat z~perzistovaného úložiště, která téměř beze změny předá klientovi. Tento přístup si vybere daň v~podobě obsáhlejší databáze, neboť budeme ukládat i~taková data, která bychom dokázali vygenerovat z~ostatních informací. Nejdůležitější instancí tohoto opakování dat bude uložení JSON-LD reprezentace a~zároveň strukturovaných dat v~rámci každého dokumentu receptu. Strukturovaná data by bylo možné při každém dotazu odvodit z JSON-LD nebo naopak. Přesunuli bychom ale komplexitu převodu z~jednorázové fáze předzpracování na samotnou aplikaci, ať už na straně serveru či klienta. Příprava dat pro prezentaci by navíc při každém dotazu trvala o~něco déle, což by se při větším množství dat mohlo negativně odrazit na svižnosti aplikace a~tranzitivně na uživatelském zážitku.

\section{Příprava dat}

V~předchozí kapitole jsme si představili řadu alternativ pro získání datasetů s~recepty a~následně doplňujících informací k~ingrediencím. Dle požadavků aplikace potřebujeme minimálně $50\,000$ receptů z~aspoň $2$ zdrojů. Také vyžadujeme integraci $2$ nebo více znalostních grafů s~otevřenými daty. Nemáme spodní limit na počet ingrediencí, které musíme ze znalostních grafů extrahovat. Záleží totiž na úspěšnosti propojení našich ingrediencí s~entitami ze znalostních grafů, která se projeví až při praktickém testu.

Přípravu dat můžeme dále rozdělit na $2$ základní fáze a~to extrakci a čištění dat. Ne všechna data jsou totiž vhodná pro přímou prezentaci uživateli. Jak jsme zmiňovali v~minulé kapitole, volně dostupné datasety s~recepty často cílí spíše na oblast strojového učení. Extrahovaná data je tedy potřeba manuálně zkontrolovat a navrhnout heuristiku, pomocí které bude většina dat normalizována, případně odstraněna při nesplnění zadaných kritérií. Vzhledem k charakteru problému a~omezené časové dotaci nelze na každou datovou sadu aplikovat deterministické řešení, které by eliminovalo všechny anomálie, proto se v~některých případech musíme spokojit s~heuristikou.

\subsection{Extrakce dat}

Na tomto místě je vhodné rozhodnout, které ze zdrojů dat popsaných v předchozí kapitole nakonec použijeme v~rámci našeho řešení. U~dat k~ingrediencím máme na výběr znalostní grafy DBpedia a~Wikidata, případně méně obsáhlý RDF dataset z~projektu FoodKG. Pro maximalizaci počtu nalezených výsledků se zaměříme na grafy DBpedia a~Wikidata. V~kategorii receptů zvolíme kombinaci statických datových sad s~recepty a~generování vlastních datasetů pomocí procesu web scraping. Během implementace vlastní extrakce dat zapojíme knihovnu Apify pro Node.js a~její koncept tzv. \emph{actorů}, což jsou programy určené primárně pro cloudovou platformu Apify, kde jsou spouštěny uvnitř Docker kontejnerů. Mohou mít za úkol automatizaci libovolných úkonů prováděných ve webovém prohlížeči, od jednoduchého posílání e-mailů až po extrakci dat z~komplexních webových stránek. Actory lze pomocí Apify CLI spouštět i~lokálně, čehož pro jednodušší konfiguraci využijeme v~našem řešení. Volitelně lze aktivovat rotování IP adres, které chrání naši vlastní IP adresu před dočasným či dokonce trvalým zablokováním a~zlepšuje poměr úspěšných requestů. Vzhledem k~obecně nižší míře blokování ze strany aplikací s~recepty by využití proxy nemělo být nutné, je ale doporučeno. Počet současně odesílaných requestů omezíme na doporučenou hranici $50$ requestů, čímž bychom měli předejít přetížení zpracovávané webové aplikace.

\subsubsection{Food.com}

Vzhledem k~požadavkům definovaným v~předchozí kapitole nám bude vyhovovat dataset Food.com Recipes and Interactions dostupný na platformě Kaggle, z~něhož jsme schopni získat přibližně $180\,000$ identifikátorů receptů a~také seznam normalizovaných ingrediencí. Dle provedené analýzy není vhodné použít textová data v~prezentační vrstvě vzhledem k jejich lowercase formátu. Navrhneme tedy řešení z~oblasti web scrapingu, které na vstupu přijme url adresy s~detaily receptů, pošle na každé ze zadaných url GET request a~z~HTML odpovědi extrahuje JSON-LD data. Program bude mít možnost získat přes CSS selektory libovolná data z~načteného HTML, pokud by v JSON-LD reprezentaci nebyla obsažena, nebo byla méně strukturována. Programu tedy přidělíme také zodpovědnost za tvorbu strukturovaných dat, která do vygenerovaného datasetu uloží ke každému receptu spolu s~jeho JSON-LD podobou. Strukturovanými daty zde rozumíme čas přípravy, počet porcí, klíčová slova, která jsou v~JSON-LD uložena ve společném řetězci namísto pole řetězců, hodnocení receptu s~počtem recenzí, nutriční hodnoty s~jednotkami měření a~ingredience s~množstvím (případně i~jednotkou) odděleným od ostatního textu.

Extrahované výsledky uložíme do společného JSON souboru, který následně sloučíme s~vybranými informacemi z~datasetu Food.com Recipes and Interactions. JSON-LD např. neobsahuje kompletní informace o~autorovi, ale pouze jeho jméno. Dle samotného jména nejsme schopni autora jednoznačně identifikovat a~zjistit odkaz na jeho profil v~rámci aplikace Food.com. Url adresa autora je totiž sestavena z~jeho unikátního id, které máme k~dispozici právě v~datasetu z~Kaggle. Dále budeme chtít extrahované recepty rozšířit o~normalizované ingredience, abychom nemuseli navrhovat vlastní heuristiku a~usnadnili si pozdější mapování ingrediencí na entity ze znalostních grafů. Po sloučení všech potřebných dat provedeme finální čištění a následně recepty jako JSON dokumenty uložíme do databáze.

Výše popsané řešení extrakce dat z~Food.com má nevýhodu z~pohledu škálovatelnosti. Maximální počet receptů, které jsme schopni získat, je roven počtu receptů v~datasetu z~Kaggle. Celkový počet receptů na stránce Food.com se od doby pořízení datasetu zvětšil více než dvakrát na aktuálních $526\,851$ receptů. Nicméně i~s~naším zjednodušeným programem vyžadujícím připravené url adresy detailů receptů jsme schopni získat téměř kompletní data. Zmíněný dataset Recipe1M+ v~době psaní této práce obsahuje téměř $510\,000$ url adres receptů z~aplikace Food.com. Při potřebě většího škálování bychom mohli využít tato url, neměli bychom k~nim ovšem normalizované ingredience a~byli bychom omezeni striktně akademickým využitím. Pro účely naší práce se spokojíme s~horní hranicí $180\,000$ receptů s~normalizovanými ingrediencemi. Tyto recepty jsou dle autorů datasetu Majumdera a~kol. podmnožinou receptů z~let $2000$-$2018$, které mají aspoň $3$~kroky postupu a~počet ingrediencí v rozmezí $4$ a $20$ \citep{majumder-etal-2019-generating}. Kód souvisejícího projektu pro generování personalizovaných receptů je dostupný jako open-source na platformě GitHub, lze tedy předpokládat, že datovou sadu lze využívat bez omezení.

\subsubsection{Allrecipes}

Jako další zdroj receptů si vybereme webovou aplikaci Allrecipes. Pro ni sice nemáme k~dispozici podrobný dataset jako u~stránky Food.com, vystačíme si ale s~vlastní extrakcí dat prostřednictvím Apify actora. Mohli bychom využít prakticky stejnou šablonu, jako u~programu pro zpracování Food.com. S~využitím datasetu Recipe1M+ dokážeme získat $49\,000$ url adres detailů receptů. Poměrně snadno bychom ale dokázali navrhnout komplexnější řešení extrakce dat, které by dynamicky procházelo celou webovou stránku Allrecipes, našlo detaily všech receptů a~z~nich extrahovalo aktuální data. Tímto přístupem bychom odstranili závislost na datové sadě Recipe1M+ a~získali větší počet výsledků. Pro nalezení všech receptů bychom sice museli zpracovat více požadavků, aplikace Allrecipes ale využívá interní API, přes které lze získat url adresy $48$ receptů v~rámci $1$ requestu. Celkový počet receptů na Allrecipes se aktuálně pohybuje kolem $50\,000$, což lze zjistit spuštěním vyhledávání bez jakýchkoli nastavených filtrů.

Aplikace Allrecipes nabízí svým uživatelům vyhledávání dle ingrediencí a~také možnost přizpůsobit množství ingrediencí dle požadovaného počtu porcí. Tato skutečnost naznačuje, že si aplikace interně spravuje ingredience ve strukturované podobě, přestože v~přiloženém JSON-LD je poskytuje jako prostý text včetně množství a jednotky měření. Zaměřme se na konkrétní ingredienci uvnitř HTML dokumentu vybraného receptu. Můžeme si povšimnout, že jsou v~atributech příslušného \texttt{input} elementu uložena strukturovaná data ingredience v~následujícím formátu (ukázka z~receptu \texttt{92462} pro surovinu kuřecí vývar):

\begin{code}
<input
    class="checkbox-list-input"
    data-tracking-label="ingredient clicked"
    data-quantity="½"
    data-init-quantity="0.5"
    data-unit="cup"
    data-ingredient="chicken broth"
    data-unit_family="volumetric"
    data-store_location="Soup"
    type="checkbox"
    value="(14.5 ounce) can chicken broth"
    id="recipe-ingredients-label-92462-0-4">
\end{code}
%$

Tyto užitečné informace v~rámci extraktoru zacílíme pomocí CSS selektorů. Díky tomu získáme výrazně přesnější data, než prostřednictvím normalizovaných ingrediencí z~datasetu Food.com Recipes and~Interactions.

\subsubsection{DBpedia}

V~první fázi extrakce dat z~grafu DBpedia potřebujeme identifikovat entity ingrediencí, které dokážeme namapovat na jména surovin z~jednotlivých receptů. K~tomu využijeme nástroj Silk Workbench a~vytvoříme RDF tvrzení s~IRI adresami ingrediencí spojenými vztahem \texttt{owl:sameAs}. V rámci úlohy linkování navrhneme tranformaci textu ingrediencí, která dokáže názvy propojit i s mírnými odlišnostmi ve formátu, čísle nebo pádu slov. Pro každou ingredienci vyjádřenou pomocí DBpedia IRI pak extrahujeme vybrané informace včetně názvu, popisu, obrázku, kategorií a~místa původu. Z~nutričních hodnot se zaměříme na energii v~kaloriích nebo kilojoulech, dále na obsah tuku, sacharidů, bílkovin, vlákniny, cholesterolu a~cukru. Aktuálně se zabýváme pouze anglickou lokalizací aplikace, všechna textová data tedy omezíme na anglické výsledky. Jedinou povinnou informací bude název (label) ingredience, všechna ostatní data budou nepovinná, neboť se formát i~množství dat napříč ingrediencemi výrazně liší.

Teoreticky bychom mohli vytvořit jeden společný SPARQL dotaz pro všechny ingredience a ten odeslat na DBpedia SPARQL endpoint. Dotaz by ale v~závislosti na počtu nalezených linků mezi surovinami mohl skončit příliš dlouhý a~nenechal by prostor pro škálování. Zvolíme tedy alternativní řešení --- dynamicky vytvoříme sadu dotazů stejného formátu, každý s~přibližně $20$ IRI adresami entit ingrediencí. Tyto dotazy zpracujeme postupně a~výsledky uložíme do společného JSON datasetu s~detaily ingrediencí. Výsledky si navíc od SPARQL endpointu můžeme vyžádat v~řadě různých formátů. Pro naše účely bude nejpraktičtější formát JSON-LD, jehož obsah využijeme v~hlavičkách HTML dokumentů ingrediencí. Se SPARQL endpointem lze komunikovat přes grafické rozhraní ve webovém prohlížeči nebo prostřednictvím HTTP GET requestů. Pro snadnější automatizaci procesu extrakce využijeme druhou možnost, kde obsah dotazu předáme na místě query parametru s názvem \texttt{query}.

\subsubsection{Wikidata}

IRI adresy požadovaných entit z~Wikidata získáme opět pomocí aplikace Silk Workbench. Také samotný proces extrakce dat bude probíhat analogicky k~postupu pro data z~DBpedia. I~zde využijeme HTTP GET requesty na SPARQL endpoint, kde prostřednictvím query parametrů předáme obsah dotazu a~požadovaný formát výsledku. Projekt Wikidata neposkytuje reprezentaci JSON-LD, vystačíme si ale s~běžným JSON formátem, který lze vyžádat přes hodnotu query parametru \texttt{format} nastavenou na \texttt{json}. Z~této reprezentace pak sami vytvoříme odpovídající JSON-LD formát, který je vhodný pro strukturovaná data v~hlavičce HTML dokumentu.

\subsection{Čištění dat}

V~rámci fáze čištění dat potřebujeme extrahovaná data převést do formátu vhodného k~prezentaci koncovému uživateli. Jednotlivé kroky procesu čištění mohou být rozloženy do více míst přípravy dat. Již během extrakce dat probíhá odstranění mezer a~znaků nového řádku na okrajích řetězců. Dále je potřeba se vypořádat se znaky, které jsou kvůli vnoření v~HTML dokumentu kódovány jinými znaky, aby bylo zajištěno jejich korektní zobrazení. Takové znaky se vyskytují např. v~extrahovaných JSON-LD dokumentech, před jejich uložením do databáze tedy provedeme dekódování. Rekurzivně projdeme obsah každého objektu načteného z~JSON-LD dokumentu a~všechny řetězce dekódujeme s~využitím open-source knihoven pro Node.js. V~našem řešení integrujeme knihovny \texttt{html-escaper} a~\texttt{html-entities} dostupné přes správce balíčků npm.

Dále jsme se rozhodli z~vyhledávání vyřadit recepty bez fotografie, které lze identifikovat a~přeskočit již během fáze extrakce nebo následně při ukládání do databáze, případně až při tvorbě dokumentů pro vyhledávací platformu Solr. Zvolíme poslední způsob, recepty tedy uložíme do vlastní databáze bez ohledu na přítomnost jejich obrázků. Díky tomu budeme mít v~budoucnu snadnou cestu k~využití zbývajících receptů bez fotografií, ať už pro účely strojového učení nebo i~zobrazení uživateli, pokud by větší nabídka receptů výrazně převážila nevýhodu absence ilustračních fotografií.

Také data k~ingrediencím budou vyžadovat významné čištění. V~datasetu Food.com~Recipes and~Interactions máme k~dispozici přibližně $8\,000$ unikátních ingrediencí. Co nejvíce z~nich bychom chtěli nabídnout uživateli v~rámci našeptávače ve vyhledávání dle ingrediencí. Pro tento účel názvy ingrediencí převedeme do estetičtějšího formátu s~velkým počátečním písmenem. Po manuální kontrole seznamu ingrediencí ale narazíme na řadu slov, která se mezi suroviny dostala omylem vlivem chybného parsování jmen ingrediencí. Nebudeme zde uvádět kompletní výčet, typicky se ale jedná o názvy jednotek měření nebo obecné fragmenty ingrediencí, které samy o~sobě žádnou ingredienci nepředstavují (např. samostatná slova \texttt{clove}, \texttt{seed}, \texttt{extract}, která by byla validní pouze v~kontextu typu \texttt{garlic clove}, \texttt{sesame seed} a~\texttt{vanilla extract}). Vzhledem k~velkému počtu ingrediencí navrhneme heuristiku čištění pomocí regulárních výrazů. Zaměříme se zejména na nejčastěji používané ingredience, které budou zobrazeny v~horní části našeptávače. Pro potřeby našeptávače nastavíme limit maximálního počtu slov ingredience a~to na hodnotu $3$. Pro mapování na entity ze znalostních grafů ale využijeme původní sadu ingrediencí bez omezení počtu slov.

S~normalizovanými ingrediencemi z~Food.com Recipes and~Interactions souvisí další problém --- nejsou přiřazeny ke všem receptům z~datasetu. Texty surovin sice využijeme z~extrahovaného JSON-LD, normalizované ingredience ale potřebujeme k~propojení s~informacemi z~grafů DBpedia a~Wikidata. Recepty bez normalizovaných ingrediencí tedy musíme projít a~pro každou jejich přísadu zkusit na základě prostého textu nalézt co nejbližší shodu s~některou z~normalizovaných ingrediencí. Pro zjednodušení budeme akceptovat pouze přesné shody, přestože nám tímto způsobem může část ingrediencí uniknout, neboť mohou být v~prostém textu uvedeny v~jiném pádě nebo čísle.

Dalším úkolem čisticí fáze bude normalizace JSON-LD reprezentace ingrediencí z~grafu DBpedia. Oproti projektu Wikidata zde máme výhodu, jelikož data obdržíme přímo v JSON-LD. Zároveň ale extrahujeme data pro více ingrediencí najednou a~každá z~nich má vlastní schéma, které se většinou plně neshoduje s~ostatními entitami. Při skupinové extrakci dat se ale musí vytvořit univerzální schéma, kterým lze vyjádřit všechny obsažené informace. Naším úkolem bude projít uložené kolekce ingrediencí a~pro každou ingredienci vytvořit minimální JSON-LD kontext, kterým ji lze popsat. Jednotlivé ingredience pak do databáze uložíme vždy s~vlastním JSON-LD kontextem. V~praxi se totiž často stává, že objevíme pod stejnou vlastností různé typy hodnot. Např. region původu ingredience může nést IRI příslušné entity z~grafu DBpedia, ale také prostý literál. Skutečný typ musí být řádně definován kontextem JSON-LD dokumentu. Proto není vhodné mít společný kontext pro všechny ingredience, neboť by u~některých vlastností existoval duplicitní popis použitých typů.

\section{Databázový model}

Pro uložení dat zvolíme dokumentovou databázi, konkrétně Apache CouchDB. 


\section{Indexy}



\section{Backend}


\section{Frontend}


\section{Aplikační logika}