%%% Fiktivní kapitola s ukázkami citací

\chapter{Architektura řešení}

V~této kapitole se budeme věnovat návrhu architektury od extrakce a~uložení dat přes přípravu vyhledávacích indexů až po prezentaci těchto dat v~rámci funkční aplikace. Předzpracování dat bude klíčovou fází řešení spolu s~prezentační vrstvou na straně klienta. Serverová část aplikace bude vystupovat pouze jako prostředník mezi frontendem a~databází, respektive platformou Solr pro vyhledávací dotazy. Jejím úkolem bude zprostředkování požadovaných dat z~perzistovaného úložiště, která téměř beze změny předá klientovi. Tento přístup si vybere daň v~podobě obsáhlejší databáze, neboť budeme ukládat i~taková data, která bychom dokázali vygenerovat z~ostatních informací. Nejdůležitější instancí tohoto opakování dat bude uložení JSON-LD reprezentace a~zároveň strukturovaných dat v~rámci každého dokumentu receptu. Strukturovaná data by bylo možné při každém dotazu odvodit z JSON-LD nebo naopak. Přesunuli bychom ale komplexitu převodu z~jednorázové fáze předzpracování na samotnou aplikaci, ať už na straně serveru či klienta. Příprava dat pro prezentaci by navíc při každém dotazu trvala o~něco déle, což by se při větším množství dat mohlo negativně odrazit na svižnosti aplikace a~tranzitivně na uživatelském zážitku.

\section{Příprava dat}

V~předchozí kapitole jsme si představili řadu alternativ pro získání datasetů s~recepty a~následně doplňujících informací k~ingrediencím. Dle požadavků aplikace potřebujeme minimálně $50\,000$ receptů z~aspoň $2$ zdrojů. Také vyžadujeme integraci $2$ nebo více znalostních grafů s~otevřenými daty. Nemáme spodní limit na počet ingrediencí, které musíme ze znalostních grafů extrahovat. Záleží totiž na úspěšnosti propojení našich ingrediencí s~entitami ze znalostních grafů, která se projeví až při praktickém testu.

Přípravu dat můžeme dále rozdělit na $2$ základní fáze a~to extrakci a čištění dat. Ne všechna data jsou totiž vhodná pro přímou prezentaci uživateli. Jak jsme zmiňovali v~minulé kapitole, volně dostupné datasety s~recepty často cílí spíše na oblast strojového učení. Extrahovaná data je tedy potřeba manuálně zkontrolovat a navrhnout heuristiku, pomocí které bude většina dat normalizována, případně odstraněna při nesplnění zadaných kritérií. Vzhledem k charakteru problému a~omezené časové dotaci nelze na každou datovou sadu aplikovat deterministické řešení, které by eliminovalo všechny anomálie, proto se v~některých případech musíme spokojit s~heuristikou.

\subsection{Extrakce dat}

Na tomto místě je vhodné rozhodnout, které ze zdrojů dat popsaných v předchozí kapitole nakonec použijeme v~rámci našeho řešení. U~dat k~ingrediencím máme na výběr znalostní grafy DBpedia a~Wikidata, případně méně obsáhlý RDF dataset z~projektu FoodKG. Pro maximalizaci počtu nalezených výsledků se zaměříme na grafy DBpedia a~Wikidata. V~kategorii receptů zvolíme kombinaci statických datových sad s~recepty a~generování vlastních datasetů pomocí procesu web scraping. Během implementace vlastní extrakce dat zapojíme knihovnu Apify pro Node.js a~její koncept tzv. \emph{actorů}, což jsou programy určené primárně pro cloudovou platformu Apify, kde jsou spouštěny uvnitř Docker kontejnerů. Mohou mít za úkol automatizaci libovolných úkonů prováděných ve webovém prohlížeči, od jednoduchého posílání e-mailů až po extrakci dat z~komplexních webových stránek. Actory lze pomocí Apify CLI spouštět i~lokálně, čehož pro jednodušší konfiguraci využijeme v~našem řešení. Volitelně lze aktivovat rotování IP adres, které chrání naši vlastní IP adresu před dočasným či dokonce trvalým zablokováním a~zlepšuje poměr úspěšných requestů. Vzhledem k~obecně nižší míře blokování ze strany aplikací s~recepty by využití proxy nemělo být nutné, je ale doporučeno. Počet současně odesílaných requestů omezíme na doporučenou hranici $50$ requestů, čímž bychom měli předejít přetížení zpracovávané webové aplikace.

\subsubsection{Food.com}

Vzhledem k~požadavkům definovaným v~předchozí kapitole nám bude vyhovovat dataset Food.com Recipes and Interactions dostupný na platformě Kaggle, z~něhož jsme schopni získat přibližně $180\,000$ identifikátorů receptů a~také seznam normalizovaných ingrediencí. Dle provedené analýzy není vhodné použít textová data v~prezentační vrstvě vzhledem k jejich lowercase formátu. Navrhneme tedy řešení z~oblasti web scrapingu, které na vstupu přijme url adresy s~detaily receptů, pošle na každé ze zadaných url GET request a~z~HTML odpovědi extrahuje JSON-LD data. Program bude mít možnost získat přes CSS selektory libovolná data z~načteného HTML, pokud by v JSON-LD reprezentaci nebyla obsažena, nebo byla méně strukturována. Programu tedy přidělíme také zodpovědnost za tvorbu strukturovaných dat, která do vygenerovaného datasetu uloží ke každému receptu spolu s~jeho JSON-LD podobou. Strukturovanými daty zde rozumíme čas přípravy, počet porcí, klíčová slova, která jsou v~JSON-LD uložena ve společném řetězci namísto pole řetězců, hodnocení receptu s~počtem recenzí, nutriční hodnoty s~jednotkami měření a~ingredience s~množstvím (případně i~jednotkou) odděleným od ostatního textu.

Extrahované výsledky uložíme do společného JSON souboru, který následně sloučíme s~vybranými informacemi z~datasetu Food.com Recipes and Interactions. JSON-LD např. neobsahuje kompletní informace o~autorovi, ale pouze jeho jméno. Dle samotného jména nejsme schopni autora jednoznačně identifikovat a~zjistit odkaz na jeho profil v~rámci aplikace Food.com. Url adresa autora je totiž sestavena z~jeho unikátního id, které máme k~dispozici právě v~datasetu z~Kaggle. Dále budeme chtít extrahované recepty rozšířit o~normalizované ingredience, abychom nemuseli navrhovat vlastní heuristiku a~usnadnili si pozdější mapování ingrediencí na entity ze znalostních grafů. Po sloučení všech potřebných dat provedeme finální čištění a následně recepty jako JSON dokumenty uložíme do databáze.

Výše popsané řešení extrakce dat z~Food.com má nevýhodu z~pohledu škálovatelnosti. Maximální počet receptů, které jsme schopni získat, je roven počtu receptů v~datasetu z~Kaggle. Celkový počet receptů na stránce Food.com se od doby pořízení datasetu zvětšil více než dvakrát na aktuálních $526\,851$ receptů. Nicméně i~s~naším zjednodušeným programem vyžadujícím připravené url adresy detailů receptů jsme schopni získat téměř kompletní data. Zmíněný dataset Recipe1M+ v~době psaní této práce obsahuje téměř $510\,000$ url adres receptů z~aplikace Food.com. Při potřebě většího škálování bychom mohli využít tato url, neměli bychom k~nim ovšem normalizované ingredience a~byli bychom omezeni striktně akademickým využitím. Pro účely naší práce se spokojíme s~horní hranicí $180\,000$ receptů s~normalizovanými ingrediencemi. Tyto recepty jsou dle autorů datasetu Majumdera a~kol. podmnožinou receptů z~let $2000$-$2018$, které mají aspoň $3$~kroky postupu a~počet ingrediencí v rozmezí $4$ a $20$ \citep{majumder-etal-2019-generating}. Kód souvisejícího projektu pro generování personalizovaných receptů je dostupný jako open-source na platformě GitHub, lze tedy předpokládat, že datovou sadu lze využívat bez omezení.

\subsubsection{Allrecipes}

Jako další zdroj receptů si vybereme webovou aplikaci Allrecipes. Pro ni sice nemáme k~dispozici podrobný dataset jako u~stránky Food.com, vystačíme si ale s~vlastní extrakcí dat prostřednictvím Apify actora. Mohli bychom využít prakticky stejnou šablonu, jako u~programu pro zpracování Food.com. S~využitím datasetu Recipe1M+ dokážeme získat $49\,000$ url adres detailů receptů. Poměrně snadno bychom ale dokázali navrhnout komplexnější řešení extrakce dat, které by dynamicky procházelo celou webovou stránku Allrecipes, našlo detaily všech receptů a~z~nich extrahovalo aktuální data. Tímto přístupem bychom odstranili závislost na datové sadě Recipe1M+ a~získali větší počet výsledků. Pro nalezení všech receptů bychom sice museli zpracovat více požadavků, aplikace Allrecipes ale využívá interní API, přes které lze získat url adresy $48$ receptů v~rámci $1$ requestu. Celkový počet receptů na Allrecipes se aktuálně pohybuje kolem $50\,000$, což lze zjistit spuštěním vyhledávání bez jakýchkoli nastavených filtrů.

Aplikace Allrecipes nabízí svým uživatelům vyhledávání dle ingrediencí a~také možnost přizpůsobit množství ingrediencí dle požadovaného počtu porcí. Tato skutečnost naznačuje, že si aplikace interně spravuje ingredience ve strukturované podobě, přestože v~přiloženém JSON-LD je poskytuje jako prostý text včetně množství a jednotky měření. Zaměřme se na konkrétní ingredienci uvnitř HTML dokumentu vybraného receptu. Můžeme si povšimnout, že jsou v~atributech příslušného \texttt{input} elementu uložena strukturovaná data ingredience v~následujícím formátu (ukázka z~receptu \texttt{92462} pro surovinu kuřecí vývar):

\begin{code}
<input
    class="checkbox-list-input"
    data-tracking-label="ingredient clicked"
    data-quantity="½"
    data-init-quantity="0.5"
    data-unit="cup"
    data-ingredient="chicken broth"
    data-unit_family="volumetric"
    data-store_location="Soup"
    type="checkbox"
    value="(14.5 ounce) can chicken broth"
    id="recipe-ingredients-label-92462-0-4">
\end{code}
%$

Tyto užitečné informace v~rámci extraktoru zacílíme pomocí CSS selektorů. Díky tomu získáme výrazně přesnější data, než prostřednictvím normalizovaných ingrediencí z~datasetu Food.com Recipes and~Interactions.

\subsubsection{DBpedia}

V~první fázi extrakce dat z~grafu DBpedia potřebujeme identifikovat entity ingrediencí, které dokážeme namapovat na jména surovin z~jednotlivých receptů. K~tomu využijeme nástroj Silk Workbench a~vytvoříme RDF tvrzení s~IRI adresami ingrediencí spojenými vztahem \texttt{owl:sameAs}. V rámci úlohy linkování navrhneme tranformaci textu ingrediencí, která dokáže názvy propojit i s mírnými odlišnostmi ve formátu, čísle nebo pádu slov. Pro každou ingredienci vyjádřenou pomocí DBpedia IRI pak extrahujeme vybrané informace včetně názvu, popisu, obrázku, kategorií a~místa původu. Z~nutričních hodnot se zaměříme na energii v~kaloriích nebo kilojoulech, dále na obsah tuku, sacharidů, bílkovin, vlákniny, cholesterolu a~cukru. Aktuálně se zabýváme pouze anglickou lokalizací aplikace, všechna textová data tedy omezíme na anglické výsledky. Jedinou povinnou informací bude název (label) ingredience, všechna ostatní data budou nepovinná, neboť se formát i~množství dat napříč ingrediencemi výrazně liší.

Teoreticky bychom mohli vytvořit jeden společný SPARQL dotaz pro všechny ingredience a ten odeslat na DBpedia SPARQL endpoint. Dotaz by ale v~závislosti na počtu nalezených linků mezi surovinami mohl skončit příliš dlouhý a~nenechal by prostor pro škálování. Zvolíme tedy alternativní řešení --- dynamicky vytvoříme sadu dotazů stejného formátu, každý s~přibližně $20$ IRI adresami entit ingrediencí. Tyto dotazy zpracujeme postupně a~výsledky uložíme do společného JSON datasetu s~detaily ingrediencí. Výsledky si navíc od SPARQL endpointu můžeme vyžádat v~řadě různých formátů. Pro naše účely bude nejpraktičtější formát JSON-LD, jehož obsah využijeme v~hlavičkách HTML dokumentů ingrediencí. Se SPARQL endpointem lze komunikovat přes grafické rozhraní ve webovém prohlížeči nebo prostřednictvím HTTP GET requestů. Pro snadnější automatizaci procesu extrakce využijeme druhou možnost, kde obsah dotazu předáme na místě query parametru s názvem \texttt{query}.

\subsubsection{Wikidata}

IRI adresy požadovaných entit z~Wikidata získáme opět pomocí aplikace Silk Workbench. Také samotný proces extrakce dat bude probíhat analogicky k~postupu pro data z~DBpedia. I~zde využijeme HTTP GET requesty na SPARQL endpoint, kde prostřednictvím query parametrů předáme obsah dotazu a~požadovaný formát výsledku. Projekt Wikidata neposkytuje reprezentaci JSON-LD, vystačíme si ale s~běžným JSON formátem, který lze vyžádat přes hodnotu query parametru \texttt{format} nastavenou na \texttt{json}. Z~této reprezentace pak sami vytvoříme odpovídající JSON-LD formát, který je vhodný pro strukturovaná data v~hlavičce HTML dokumentu.

\subsection{Čištění dat}

V~rámci fáze čištění dat potřebujeme extrahovaná data převést do formátu vhodného k~prezentaci koncovému uživateli. Jednotlivé kroky procesu čištění mohou být rozloženy do více míst přípravy dat. Již během extrakce dat probíhá odstranění mezer a~znaků nového řádku na okrajích řetězců. Dále je potřeba se vypořádat se znaky, které jsou kvůli vnoření v~HTML dokumentu kódovány jinými znaky, aby bylo zajištěno jejich korektní zobrazení. Takové znaky se vyskytují např. v~extrahovaných JSON-LD dokumentech, před jejich uložením do databáze tedy provedeme dekódování. Rekurzivně projdeme obsah každého objektu načteného z~JSON-LD dokumentu a~všechny řetězce dekódujeme s~využitím open-source knihoven pro Node.js. V~našem řešení integrujeme knihovny \texttt{html-escaper} a~\texttt{html-entities} dostupné přes správce balíčků npm.

Dále jsme se rozhodli z~vyhledávání vyřadit recepty bez fotografie, které lze identifikovat a~přeskočit již během fáze extrakce nebo následně při ukládání do databáze, případně až při tvorbě dokumentů pro vyhledávací platformu Solr. Zvolíme poslední způsob, recepty tedy uložíme do vlastní databáze bez ohledu na přítomnost jejich obrázků. Díky tomu budeme mít v~budoucnu snadnou cestu k~využití zbývajících receptů bez fotografií, ať už pro účely strojového učení nebo i~zobrazení uživateli, pokud by větší nabídka receptů výrazně převážila nevýhodu absence ilustračních fotografií.

Také data k~ingrediencím budou vyžadovat významné čištění. V~datasetu Food.com~Recipes and~Interactions máme k~dispozici přibližně $8\,000$ unikátních ingrediencí. Co nejvíce z~nich bychom chtěli nabídnout uživateli v~rámci našeptávače ve vyhledávání dle ingrediencí. Pro tento účel názvy ingrediencí převedeme do estetičtějšího formátu s~velkým počátečním písmenem. Po manuální kontrole seznamu ingrediencí ale narazíme na řadu slov, která se mezi suroviny dostala omylem vlivem chybného parsování jmen ingrediencí. Nebudeme zde uvádět kompletní výčet, typicky se ale jedná o názvy jednotek měření nebo obecné fragmenty ingrediencí, které samy o~sobě žádnou ingredienci nepředstavují (např. samostatná slova \texttt{clove}, \texttt{seed}, \texttt{extract}, která by byla validní pouze v~kontextu typu \texttt{garlic clove}, \texttt{sesame seed} a~\texttt{vanilla extract}). Vzhledem k~velkému počtu ingrediencí navrhneme heuristiku čištění pomocí regulárních výrazů. Zaměříme se zejména na nejčastěji používané ingredience, které budou zobrazeny v~horní části našeptávače. Pro potřeby našeptávače nastavíme limit maximálního počtu slov ingredience a~to na hodnotu $3$. Pro mapování na entity ze znalostních grafů ale využijeme původní sadu ingrediencí bez omezení počtu slov.

S~normalizovanými ingrediencemi z~Food.com Recipes and~Interactions souvisí další problém --- nejsou přiřazeny ke všem receptům z~datasetu. Texty surovin sice využijeme z~extrahovaného JSON-LD, normalizované ingredience ale potřebujeme k~propojení s~informacemi z~grafů DBpedia a~Wikidata. Recepty bez normalizovaných ingrediencí tedy musíme projít a~pro každou jejich přísadu zkusit na základě prostého textu nalézt co nejbližší shodu s~některou z~normalizovaných ingrediencí. Pro zjednodušení budeme akceptovat pouze přesné shody, přestože nám tímto způsobem může část ingrediencí uniknout, neboť mohou být v~prostém textu uvedeny v~jiném pádě nebo čísle.

Dalším úkolem čisticí fáze bude normalizace JSON-LD reprezentace ingrediencí z~grafu DBpedia. Oproti projektu Wikidata zde máme výhodu, jelikož data obdržíme přímo v JSON-LD. Zároveň ale extrahujeme data pro více ingrediencí najednou a~každá z~nich má vlastní schéma, které se většinou plně neshoduje s~ostatními entitami. Při skupinové extrakci dat se ale musí vytvořit univerzální schéma, kterým lze vyjádřit všechny obsažené informace. Naším úkolem bude projít uložené kolekce ingrediencí a~pro každou ingredienci vytvořit minimální JSON-LD kontext, kterým ji lze popsat. Jednotlivé ingredience pak do databáze uložíme vždy s~vlastním JSON-LD kontextem. V~praxi se totiž často stává, že objevíme pod stejnou vlastností různé typy hodnot. Např. region původu ingredience může nést IRI příslušné entity z~grafu DBpedia, ale také prostý literál. Skutečný typ musí být řádně definován kontextem JSON-LD dokumentu. Proto není vhodné mít společný kontext pro všechny ingredience, neboť by u~některých vlastností existoval duplicitní popis použitých typů.

\section{Databázový model}

Pro uložení dat receptů a~ingrediencí zvolíme dokumentovou databázi Apache CouchDB, často označovanou zkráceně CouchDB. Nejbližší alternativou z~řad NoSQL databází by byl databázový systém MongoDB, který je stejně jako databáze CouchDB open-source. Pro naše potřeby by dobře fungoval libovolný z~těchto systémů, neboť chceme využít zejména konceptu dokumentových databází, které typicky nevyžadují striktní schéma a~umožňují tak kompaktní uložení různorodých dat ve formátu JSON. To je v~naší doméně cenná vlastnost, neboť plánujeme ukládat data poměrně komplexní struktury --- vezměme v~potaz například JSON-LD formát s~mnoha zanořenými objekty. Také extrahujeme data z~více zdrojů a~vyžadování zcela jednotného rozhraní by nám v~některých situacích zbytečně zkomplikovalo práci. Na základě jakého kritéria tedy rozhodujeme mezi variantami CouchDB a~MongoDB?

Databázi budeme pokládat pouze velmi jednoduché dotazy, totiž vyžádání dokumentu na základě jeho unikátního id. Pro práci s~databází preferujeme použití Node.js knihovny, což je splnitelné oběma databázovými systémy CouchDB i~MongoDB, dokonce prostřednictvím oficiálních ovladačů pro Node.js. K~obsluze složitějších vyhledávacích dotazů využijeme systém Apache Solr, který si bude držet vlastní podmnožinu dat z~dokumentů uložených v~databázi. Kombinací Apache CouchDB s~Apache Solr bychom sjednotili technologie z~dílny Apache Software Foundation. Zároveň by se nám v~budoucnu mohla hodit podpora současného čtení a~zápisu, kterou systém CouchDB nabízí \citep{mongodb-vs-couchdb}. Co se týče dat s~recepty či ingrediencemi, není potřeba vyžadovat, aby se uživateli vždy zobrazila verze se všemi aktualizacemi. Např. vybraný recept zůstává relevantní i~v~momentě, kdy zrovna současný nebo jiný uživatel přidává k~receptu nové hodnocení. Díky systému verzování dokumentů, který CouchDB implementuje, se nemusíme obávat o~dostupnost dat během jejich aktualizace.

V~systému CouchDB lze snadno vytvářet a~spravovat více databází neboli kolekcí dokumentů. Vytvoříme tedy samostatné databáze pro recepty a~ingredience, přičemž oba typy dokumentů v~sobě budou mít uložena strukturovaná data i~JSON-LD reprezentaci. Nemusíme předem definovat žádná schémata ukládaných dat, jako tomu bývá u~tradičních relačních databází.

S~CouchDB můžeme komunikovat přes webové rozhraní skrze aplikaci Fauxton, pomocí REST API nebo prostřednictvím již zmíněné knihovny v~prostředí Node.js. Pro automatizované nahrávání dokumentů využijeme oficiální knihovnu \texttt{nano}. S~velikostí našeho projektu si můžeme dovolit při nahrávání nových dat nejprve stávající data odstranit a~poté je vložit do čisté databáze. Kdybychom totiž chtěli dokumenty aktualizovat, potřebovali bychom u~každého z nich poskytnout aktuální verzi, což by pro přepsání celé databáze byl zbytečně komplikovaný postup. S~rostoucím počtem dokumentů bychom ale nejspíše museli přistoupit na aktualizaci dat namísto jejich odstraňování a~opětovného nahrávání, neboť již pro přidání kolekce o~velikosti $50\,000$ dokumentů se pohybujeme v~řádu delších minut.

\section{Indexy}

Pro uložení dokumentů do databáze CouchDB není potřeba specifikovat žádné schéma. Během konfigurace platformy Apache Solr pro vyhledávání se ovšem bez striktně definovaného schématu neobejdeme. Respektive abychom byli přesní, Solr dokáže na základě vložených dat odvodit schéma dynamicky, často ale nezvolí datový typ správně a~navíc je při výběru zbytečně generický. Představme si libovolný řetězec z~dokumentu receptu, například text jedné ingredience. Pokud Solr nenajde ve schématu žádnou definici typu u~vlastnosti ingredience, vytvoří pro ni dynamický index typu \texttt{string}. Na první pohled se zdá být vše v~pořádku, když si ale projdeme dostupné typy, najdeme mezi nimi i~podstatně specifičtější variantu \texttt{text\_en}. Typ anglického textu je pro naši aktuální lokalizaci ideální, neboť nabízí vestavěnou podporu skloňování a~časování anglických slov. V~praxi nám pomůže najít více relevantních výsledků. Uživatel může zadat například ingredienci \texttt{tomatoes}, Solr provede normalizaci vyhledávaného termínu a~vrátí recepty obsahující mezi surovinami nejen slova \texttt{tomatoes}, ale také \texttt{tomato}.

Prvním krokem pro práci s~platformou Solr je tedy kompletní návrh schématu pro dokumenty receptů. Pro vytvoření schématu využijeme skript operující nad REST~API poskytovaným systémem Solr, čímž proces automatizujeme a~usnadníme případnou migraci na jiné zařízení. Solr umožňuje data rozdělit do tzv. jader, založíme tedy samostatné jádro pro dokumenty s~recepty a~vytvoříme infrastrukturu, která umožní snadné přidání nového jádra, pokud by v~budoucnu bylo potřebné. V~rámci současných požadavků aplikace by se mohlo zdát, že budeme vyhledávací schopnosti platformy Solr potřebovat i~pro dokumenty ingrediencí, konkrétně pro řešení našeptávače surovin. Seznam doporučených přísad by se totiž měl aktualizovat na základě vstupu uživatele. Po napsání každého nového znaku se musí zobrazit pouze ingredience, které odpovídají vyhledávanému výrazu. Přirozeně nebereme v~potaz velikost písmen a~vzhledem k~výhradně anglické lokalizaci se v~současnosti nezabýváme ani diakritikou. Využití systému Solr by nám pomohlo nabídnout více relevantních ingrediencí, neboť bychom vyhledávaný výraz mohli interpretovat jako anglický text a~poradit si tak s~jeho formátem v~různých tvarech a časech. Kvůli každému napsanému znaku bychom ale museli odeslat dotaz serveru hostícímu instanci Solr, což by generovalo poměrně významné zpoždění vlivem komunikace po síti. Spokojíme se tedy s~jednodušší architekturou našeptávače --- vyžádáme si všechny ingredience najednou a~zobrazení relevantních návrhů během psaní vyřešíme na straně klienta. Jak lze ale získat seznam všech unikátních ingrediencí, pokud máme v~Solr uloženy pouze dokumenty receptů? Odpovědí je fasetové vyhledávání.

\subsection{Fasetové vyhledávání}

Fasetové vyhledávání, označované také jako fasetová navigace, je způsob interakce, během které uživatel filtruje výsledky výběrem validních hodnot fasetového klasifikačního systému. Tento styl vyhledávání nevyžaduje hierarchické uspořádání nabízených možností, díky čemuž lze filtry přidávat i~odebírat v~libovolném pořadí. Navíc uživatel často předem zná počet výsledků, které se po aplikování daného filtru zobrazí \citep{faceted-search}. V~kontextu naší aplikace se fasetová navigace hodí pro jednotlivé vlastnosti vyhledávání, jakými jsou nejen ingredience, ale také klíčová slova, kategorie, typ kuchyně, čas přípravy či hodnocení. Každý z~těchto filtrů je zcela nezávislý, není tedy žádoucí vytvářet kolem nich hierarchii. Nedávalo by smysl zpřístupnit například vyhledávání dle ingrediencí až po výběru kategorie receptu. Na druhou stranu, výběr kategorií může ovlivnit (respektive omezit) nabídku ingrediencí ve fasetové navigaci a~stejně tak volba určitých ingrediencí může vyřadit některé kategorie receptů.

Platforma Solr poskytuje přímou podporu fasetové navigace nad libovolnými položkami dokumentů. Můžeme tedy například snadno specifikovat fasetové vyhledávání nad ingrediencemi, čímž získáme list unikátních jmen surovin, které se v~celé kolekci receptů vyskytují. Je zde ovšem jisté omezení. Pokud fasetové vyhledávání spustíme přímo nad ingrediencemi, které máme uloženy pod typem anglického textu, Solr nám vrátí pouze transformovaná jména ingrediencí, tak jak je má uložena pro své interní vyhledávání. Data tohoto formátu nejsou vhodná pro prezentaci uživateli, tudíž budeme potřebovat ke každému receptu přiřadit nový seznam ingrediencí určený výhradně pro fasetové vyhledávání. Na úrovni schématu definujeme typ fasetových ingrediencí jako prostý řetězec, nad kterým se neprovedou žádné transformace. Zároveň nastavíme ve schématu tzv. \emph{copy field} z~položky zdrojových ingrediencí do položky fasetových ingrediencí. Díky tomu bude stačit během přidávání dokumentů vložit ingredience pouze do jedné položky objektu, přičemž do položky pro fasetové vyhledávání se zkopírují automaticky dle schématu.

\subsection{Zvýraznění nalezených výrazů}

Dalším konceptem, se kterým se během vyhledávání pomocí Solr setkáme, bude tzv. \emph{highlighting} neboli zvýraznění vyhledaných výrazů. Využijeme jej při prezentaci surovin jakožto primárního filtru. U~libovolného receptu na vyhledávací obrazovce lze zobrazit kompletní seznam ingrediencí. Pro lepší přehlednost zvýrazníme aktuálně vyhledávané ingredience tučným písmem a~pro přesnou identifikaci těchto pojmů využijeme vestavěnou funkci od platformy Solr. Informace o~zvýrazněných ingrediencích doručíme na frontend aplikace spolu s dokumenty receptů.

\subsection{Model receptu}

Schéma receptu navrhneme dle požadavků na vlastnosti, podle kterých potřebujeme recepty vyhledávat. Zároveň zde ale patří definice všech položek, které budeme na vyhledávací stránce zobrazovat. Teoreticky bychom mohli platformu Solr využít pouze na vyhledání identifikátorů receptů na základě zadaných indexů a~ostatní informace získat z~databáze CouchDB. Tento přístup by optimalizoval množství paměti využívané systémem Solr a~odstranil poměrně výraznou duplicitu dat. Na druhou stranu by do zpracování vyhledávacích dotazů za běhu aplikace zanesl větší komplexitu a~časovou prodlevu, která by vznikla nadbytečnou komunikací s~CouchDB. Pokud si budeme všechny potřebné informace pro vyhledávací stránku držet v~systému Solr, bude nám stačit zpracovat pouze jeden vyhledávací dotaz pro zobrazení jedné stránky receptů. Rychlé vyhledávání je klíčovou funkcionalitou naší aplikace, proto zde upřednostníme optimalizaci časové složitosti namísto paměťové.

Schéma dokumentu receptu uloženého v~Solr bude následující (jedná se pouze o~ilustrační schéma, kde typy odpovídají standardním typům dle specifikace Solr, nikoli běžně dostupným typům formátu JSON):

\begin{code}
{
    name: text_en,
    description: text_en,
    recipeCategory: text_en,
    ingredients: text_en,
    tags: text_en,
    rating: pfloat,
    reviewsCount: pint,
    stepsCount: pint,
    cookMinutes: pint,
    prepMinutes: pint,
    totalMinutes: pint,
    image: string,
    date: string,
    calories: pint,
    fat: pfloat,
    saturatedFat: pfloat,
    cholesterol: pfloat,
    sodium: pfloat,
    carbohydrate: pfloat,
    fiber: pfloat,
    sugar: pfloat,
    protein: pfloat,
}
\end{code}
%$

Ne všechny obsažené položky nutně využijeme v první verzi naší prezentační vrstvy (například datum nebo počet minut samotné přípravy či vaření pokrmu). Usnadníme ale přidávání nových funkcí typu třídění výsledků na základě data publikace nebo vyřazení receptů, kde nestačí pouhá příprava ze syrových ingrediencí a je potřeba počítat s vařením.

\section{Backend}



\section{Frontend}


\section{Aplikační logika}